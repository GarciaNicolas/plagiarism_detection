{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plagiarism detection\n",
    "Natural Language Processing - Universidad Tecnol√≥gica Nacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-proccesing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "\n",
    "from doc2docx import convert as doc2docx\n",
    "from pptx import Presentation\n",
    "from docx import Document\n",
    "from docx.opc.constants import RELATIONSHIP_TYPE as RT\n",
    "\n",
    "path = './data/'\n",
    "files = os.listdir(path)\n",
    "author_synonyms = ['nombre','nombres','apellido','apellidos','nombre y apellido','apellido y nombre','nombres y apellidos','apellidos y nombres','alumno','alumnos', 'alumna','alumne','alumnes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc', 'docx', 'pdf', 'pptx'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extensions = set([file.split('.')[-1] for file in files])\n",
    "extensions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 4 file extensions: 'doc', 'docx', 'pdf', 'pptx'. For each extension, we have to create a processing method. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing data (Transforming to .docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize\n",
    "def clean_special_characters(string):\n",
    "    \"\"\"\n",
    "    Receives a string and returns it without those specials characters which are troublesome\n",
    "    \"\"\"\n",
    "    string = string.replace('\\u200b',' ')\n",
    "    string = string.replace('\\xad',' ').replace('\\xa0',' ')\n",
    "    string = string.replace('\\t',' ')\n",
    "    string = re.sub('(\\s*\\n\\s*)+', ' \\n ', string)\n",
    "    \n",
    "    #Remove accents\n",
    "    string = re.sub(\n",
    "        r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", \n",
    "        normalize( \"NFD\", string), 0, re.I)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2docx import Converter\n",
    "\n",
    "\n",
    "def cleaning_citations(strings):\n",
    "    strings = list(set(strings))\n",
    "    \n",
    "    strings = [string for string in strings if not string.startswith('mailto:')]\n",
    "    return strings\n",
    "\n",
    "\n",
    "def clean_special_characters(string):\n",
    "    \"\"\"\n",
    "    Receives a string and returns it without those specials characters which are troublesome\n",
    "    \"\"\"\n",
    "    string = string.replace('\\u200b',' ')\n",
    "    string = string.replace('\\xad',' ').replace('\\xa0',' ')\n",
    "    string = string.replace('\\t',' ')\n",
    "    string = re.sub('(\\s*\\n\\s*)+', ' \\n ', string)\n",
    "    \n",
    "    #Remove accents\n",
    "    string = re.sub(\n",
    "        r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", \n",
    "        normalize( \"NFD\", string), 0, re.I)\n",
    "    \n",
    "    return string\n",
    "\n",
    "def text_hyperlinks(string):\n",
    "    \"\"\"\n",
    "    Return a list of hyperlinks in a string\n",
    "    \"\"\"\n",
    "\n",
    "    #Regular expression of hyperlinks\n",
    "    regex = r\"(https?://\\S+)\"\n",
    "\n",
    "    #Find all hyperlinks in a string\n",
    "    hyperlinks = re.findall(regex,string)\n",
    "    \n",
    "    return hyperlinks\n",
    "\n",
    "def pdf2docx(pdf_files):\n",
    "    filenames = [os.path.splitext(doc)[0] for doc in pdf_files]\n",
    "    path_docx = './data_temp/docxs_files/'\n",
    "    path_pdf = './data_temp/pdfs_files/'\n",
    "    for file in filenames:\n",
    "        cv = Converter(path_pdf + file + '.pdf')\n",
    "        cv.convert(path_docx + file + '.docx', multi_processing=True)\n",
    "        cv.close()\n",
    "\n",
    "def docx_hyperlinks(doc):\n",
    "    hyperlinks = []\n",
    "    rels = doc.part.rels\n",
    "    for rel in rels:\n",
    "        if rels[rel].reltype == RT.HYPERLINK:\n",
    "            hyperlinks.append(rels[rel]._target)\n",
    "    return hyperlinks     \n",
    "\n",
    "def names_in_table(tables):\n",
    "\n",
    "    names = []\n",
    "    for table in tables:\n",
    "        #Look if in the headers there is a synonym for author\n",
    "        for i, cell in enumerate(table.rows[0].cells):\n",
    "            if cell.text.lower() in author_synonyms:\n",
    "                #Looking for names in the rows\n",
    "                for row in table.rows[1:]:\n",
    "                    names.append(row.cells[i].text)\n",
    "    return names\n",
    "\n",
    "def get_headers(doc):\n",
    "    headers = []\n",
    "    section = doc.sections[0]\n",
    "    header = section.header\n",
    "    for paragraph in header.paragraphs:\n",
    "        if paragraph.text not in headers:\n",
    "            headers.append(paragraph.text)\n",
    "    return headers if headers != [''] else []\n",
    "\n",
    "def get_footers(doc):\n",
    "    footers = []\n",
    "    section = doc.sections[0]\n",
    "    footer = section.footer\n",
    "    for paragraph in footer.paragraphs:\n",
    "        if paragraph.text not in footers:\n",
    "            footers.append(paragraph.text)\n",
    "    return footers if footers != [''] else []\n",
    "\n",
    "def remove_section(string, list_sections):\n",
    "    \"\"\"\n",
    "    Receives a string and a list of strings belonging to a section and returns the string without these section\n",
    "    \"\"\"\n",
    "    for section in list_sections:\n",
    "        string = string.replace(section,'')\n",
    "    return string\n",
    "\n",
    "def read_docx(file):\n",
    "    doc = Document(file)\n",
    "    hyperlinks = []\n",
    "    text = \"\"\n",
    "\n",
    "    headers = get_headers(doc)\n",
    "    footers = get_footers(doc)\n",
    "\n",
    "    names = names_in_table(doc.tables)\n",
    "    hyperlinks += (docx_hyperlinks(doc))\n",
    "    for paragraph in doc.paragraphs:\n",
    "        paragraph_text = paragraph.text\n",
    "        paragraph_text = remove_section(paragraph_text, headers)\n",
    "        paragraph_text = remove_section(paragraph_text, footers)\n",
    "        hyperlinks += text_hyperlinks(paragraph_text)\n",
    "        text += paragraph_text + ' \\n '\n",
    "    text = clean_special_characters(text)\n",
    "\n",
    "    headers = [clean_special_characters(header) for header in headers]\n",
    "\n",
    "    hyperlinks = cleaning_citations(hyperlinks)\n",
    "    df = pd.DataFrame({'filename': path_to_filename(file), 'text': [text],'author':[names] if names else np.nan, 'citations': [hyperlinks] if hyperlinks else np.nan, 'headers': [headers] if headers else np.nan})\n",
    "    return df\n",
    "\n",
    "def path_to_filename(path):\n",
    "    \"\"\"\n",
    "    Return the file name from a path\n",
    "    \"\"\"\n",
    "    return os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "def read_pptx(path):\n",
    "    \"\"\"\n",
    "    Read a pptx file and return a dataframe with the file name, the text and the citations\n",
    "    \"\"\"\n",
    "\n",
    "    text = \"\"\n",
    "    hyperlinks = []\n",
    "\n",
    "    #Reading the document by slides \n",
    "    for slide in Presentation(path).slides:\n",
    "        #Looking for shapes which have text and extracting text and hyperlinks from them\n",
    "        for shape in slide.shapes:\n",
    "            if not shape.has_text_frame:\n",
    "                continue\n",
    "            for paragraph in shape.text_frame.paragraphs:\n",
    "                    for run in paragraph.runs:\n",
    "                        \n",
    "                        #Looking for hyperlinks in the text\n",
    "                        hyperlinks = hyperlinks + text_hyperlinks(run.text)\n",
    "                        \n",
    "                        #Cleaning special characters\n",
    "                        text = text +'. ' + run.text\n",
    "    hyperlinks = list(set(hyperlinks))\n",
    "    df = pd.DataFrame({'filename': path_to_filename(path), 'text': [text], 'citations': [hyperlinks] if hyperlinks else None})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_pdf_docs(docs, df):\n",
    "    \"\"\"\n",
    "    Receive a list of .doc and .pdf files, convert them to .docx files and read them,\n",
    "    then concatenate the new dataframes of each new .docx file into a dataframe and return it\n",
    "    \"\"\"\n",
    "\n",
    "    dot_docs = [doc for doc in docs if doc.endswith('.doc')]\n",
    "    dot_pdfs = [doc for doc in docs if doc.endswith('.pdf')]\n",
    "\n",
    "    #If there is a temporal folder, remove it\n",
    "    if os.path.isdir('./data_temp'):\n",
    "\n",
    "        #Return files to their original folder and remove the temporal folder\n",
    "        for doc in dot_docs:\n",
    "            shutil.move('./data_temp/docs_files/' + doc, path)\n",
    "        for pdf in dot_pdfs:\n",
    "            shutil.move('./data_temp/pdfs_files/' + pdf, path)\n",
    "        shutil.rmtree('./data_temp')\n",
    "\n",
    "    os.mkdir('./data_temp/')    \n",
    "    os.mkdir('./data_temp/docs_files')\n",
    "    os.mkdir('./data_temp/pdfs_files')\n",
    "    os.mkdir('./data_temp/docxs_files')\n",
    "\n",
    "    #Move files into its temporal folders\n",
    "    for doc in dot_docs:\n",
    "        shutil.move(path + doc, './data_temp/docs_files/')\n",
    "    for doc in dot_pdfs:\n",
    "        shutil.move(path + doc, './data_temp/pdfs_files/')\n",
    "\n",
    "    try:\n",
    "\n",
    "        #Converting .doc files to .docx files and then to .pdf files\n",
    "        doc2docx('./data_temp/docs_files/', './data_temp/docxs_files/')\n",
    "        time.sleep(.5)\n",
    "        pdf2docx(dot_pdfs)\n",
    "        time.sleep(.5)\n",
    "\n",
    "        #Read each .pdf file and concatenate the dataframes\n",
    "        for file in os.listdir('./data_temp/docxs_files/'):\n",
    "            file_df = read_docx('./data_temp/docxs_files/'+file)\n",
    "            df = pd.concat([df,file_df], ignore_index=True)\n",
    "\n",
    "    except:\n",
    "\n",
    "        #Return files to their original folder and remove the temporal folder\n",
    "        for doc in dot_docs:\n",
    "            shutil.move('./data_temp/docs_files/' + doc, path)\n",
    "        for doc in dot_pdfs:\n",
    "            shutil.move('./data_temp/pdfs_files/' + doc, path)\n",
    "        shutil.rmtree('./data_temp')\n",
    "        raise Exception('ERROR: Could not convert files to .pdf files and read them')\n",
    "\n",
    "    #Return files to their original folder and remove the temporal folder\n",
    "    for doc in dot_docs:\n",
    "        shutil.move('./data_temp/docs_files/' + doc, path)\n",
    "    for doc in dot_pdfs:\n",
    "        shutil.move('./data_temp/pdfs_files/' + doc, path)\n",
    "    shutil.rmtree('./data_temp')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(columns=['filename','text','citations','headers'])\n",
    "\n",
    "#Create a list of .doc files and concatenate its content to the dataframe\n",
    "docs_and_docxs = [file for file in files if file.endswith('.doc') or file.endswith('.pdf')]\n",
    "df = read_pdf_docs(docs_and_docxs, df)\n",
    "\n",
    "#Create a list of files which are not .doc\n",
    "not_docs_or_docxs = [file for file in files if file not in docs_and_docxs]\n",
    "\n",
    "#Read each file depending on its extension and concatenate its content to the dataframe\n",
    "for file in not_docs_or_docxs:\n",
    "\n",
    "    file_path = path + file\n",
    "    if file.endswith('.docx'):\n",
    "        file_df = read_docx(file_path)\n",
    "    elif file.endswith('.pptx'):\n",
    "        file_df = read_pptx(file_path)\n",
    "    else:\n",
    "        raise Exception('FILE EXTENSION NOT SUPPORTED: Only support .docx, .doc, .pdf and .pptx files')\n",
    "    \n",
    "    df = pd.concat([df,file_df], ignore_index=True)\n",
    "\n",
    "#Export the dataframe to a csv file\n",
    "df.to_csv('./data.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do:\n",
    "\n",
    "- Look for topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import ast\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from googlesearch import search\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "df = pd.read_csv('./data.csv')\n",
    "nlp = spacy.load('es_core_news_lg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>citations</th>\n",
       "      <th>headers</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lopez Tomas - TP 6 - Sistemas Emergentes</td>\n",
       "      <td>\\n Marketing en Internet y \\n Nueva Economia ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['L√ìPEZ, Tomas']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TP 0 Gabriela Gonzalez MKTG y NV Econom√≠a</td>\n",
       "      <td>\\n TRABAJO PRACTICO 0 \\n Integrante ‚Äì AnÃÉo 20...</td>\n",
       "      <td>['https://www.fundacionaquae.org/wiki-aquae/in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Gonzalez, Gabriela']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>TP N¬∞ 1 ‚Äì WIKINOMICS - Melanie Blejter</td>\n",
       "      <td>\\n MARKETING EN INTERNET Y NUEVA ECONOMIA \\n ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['NOMBRE Y APELLIDO', 'Melanie Blejter ', 'LEG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>TP N¬∞ 2 ‚Äì La larga cola - Melanie Blejter</td>\n",
       "      <td>\\n MARKETING EN INTERNET Y NUEVA ECONOMIA \\n ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['NOMBRE Y APELLIDO', 'Melanie Blejter ', 'LEG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>TP N¬∞ 3 ‚Äì The Experience Economy - Melanie Ble...</td>\n",
       "      <td>\\n MARKETING EN INTERNET Y NUEVA ECONOMIA \\n ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['NOMBRE Y APELLIDO', 'Melanie Blejter ', 'LEG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filename   \n",
       "7            Lopez Tomas - TP 6 - Sistemas Emergentes  \\\n",
       "18          TP 0 Gabriela Gonzalez MKTG y NV Econom√≠a   \n",
       "45             TP N¬∞ 1 ‚Äì WIKINOMICS - Melanie Blejter   \n",
       "46          TP N¬∞ 2 ‚Äì La larga cola - Melanie Blejter   \n",
       "47  TP N¬∞ 3 ‚Äì The Experience Economy - Melanie Ble...   \n",
       "\n",
       "                                                 text   \n",
       "7    \\n Marketing en Internet y \\n Nueva Economia ...  \\\n",
       "18   \\n TRABAJO PRACTICO 0 \\n Integrante ‚Äì AnÃÉo 20...   \n",
       "45   \\n MARKETING EN INTERNET Y NUEVA ECONOMIA \\n ...   \n",
       "46   \\n MARKETING EN INTERNET Y NUEVA ECONOMIA \\n ...   \n",
       "47   \\n MARKETING EN INTERNET Y NUEVA ECONOMIA \\n ...   \n",
       "\n",
       "                                            citations headers   \n",
       "7                                                 NaN     NaN  \\\n",
       "18  ['https://www.fundacionaquae.org/wiki-aquae/in...     NaN   \n",
       "45                                                NaN     NaN   \n",
       "46                                                NaN     NaN   \n",
       "47                                                NaN     NaN   \n",
       "\n",
       "                                               author  \n",
       "7                                    ['L√ìPEZ, Tomas']  \n",
       "18                             ['Gonzalez, Gabriela']  \n",
       "45  ['NOMBRE Y APELLIDO', 'Melanie Blejter ', 'LEG...  \n",
       "46  ['NOMBRE Y APELLIDO', 'Melanie Blejter ', 'LEG...  \n",
       "47  ['NOMBRE Y APELLIDO', 'Melanie Blejter ', 'LEG...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.author.notnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the string representation of the list back to a list\n",
    "df.loc[df.citations.notnull(), 'citations'] = df[df.citations.notnull()]['citations'].apply(ast.literal_eval)\n",
    "df.loc[df.author.notnull(), 'author'] = df[df.author.notnull()]['author'].apply(ast.literal_eval)\n",
    "df.loc[df.headers.notnull(), 'headers'] = df[df.headers.notnull()]['headers'].apply(ast.literal_eval)\n",
    "\n",
    "df.loc[df.headers.notnull(), 'headers']=df[df.headers.notnull()].headers.apply(lambda x: [header for header in x if header])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_not_author(strings):\n",
    "    messy_author_strings = ['nombre','nombres','apellido','apellidos','nombre y apellido','apellido y nombre','nombres y apellidos','apellidos y nombres','alumno','alumnos', 'alumna','alumne','alumnes','legajo','email','mail','correo electronico','e-mail']\n",
    "    new_string=[]\n",
    "    for string in strings:\n",
    "        if not(any(word in string.lower() for word in messy_author_strings) or any(char.isdigit() for char in string) or re.match('^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$',string)):\n",
    "            new_string.append(string)\n",
    "    return new_string if new_string else np.nan\n",
    "\n",
    "df.loc[df.author.notnull(),'author'] = df[df.author.notnull()]['author'].apply(delete_not_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>citations</th>\n",
       "      <th>headers</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lopez Tomas - TP 6 - Sistemas Emergentes</td>\n",
       "      <td>\\n Marketing en Internet y \\n Nueva Economia ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[L√ìPEZ, Tomas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TP 0 Gabriela Gonzalez MKTG y NV Econom√≠a</td>\n",
       "      <td>\\n TRABAJO PRACTICO 0 \\n Integrante ‚Äì AnÃÉo 20...</td>\n",
       "      <td>[https://www.fundacionaquae.org/wiki-aquae/inn...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Gonzalez, Gabriela]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>TP N¬∞ 1 ‚Äì WIKINOMICS - Melanie Blejter</td>\n",
       "      <td>\\n MARKETING EN INTERNET Y NUEVA ECONOMIA \\n ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Melanie Blejter ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>TP N¬∞ 2 ‚Äì La larga cola - Melanie Blejter</td>\n",
       "      <td>\\n MARKETING EN INTERNET Y NUEVA ECONOMIA \\n ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Melanie Blejter ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>TP N¬∞ 3 ‚Äì The Experience Economy - Melanie Ble...</td>\n",
       "      <td>\\n MARKETING EN INTERNET Y NUEVA ECONOMIA \\n ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Melanie Blejter ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filename   \n",
       "7            Lopez Tomas - TP 6 - Sistemas Emergentes  \\\n",
       "18          TP 0 Gabriela Gonzalez MKTG y NV Econom√≠a   \n",
       "45             TP N¬∞ 1 ‚Äì WIKINOMICS - Melanie Blejter   \n",
       "46          TP N¬∞ 2 ‚Äì La larga cola - Melanie Blejter   \n",
       "47  TP N¬∞ 3 ‚Äì The Experience Economy - Melanie Ble...   \n",
       "\n",
       "                                                 text   \n",
       "7    \\n Marketing en Internet y \\n Nueva Economia ...  \\\n",
       "18   \\n TRABAJO PRACTICO 0 \\n Integrante ‚Äì AnÃÉo 20...   \n",
       "45   \\n MARKETING EN INTERNET Y NUEVA ECONOMIA \\n ...   \n",
       "46   \\n MARKETING EN INTERNET Y NUEVA ECONOMIA \\n ...   \n",
       "47   \\n MARKETING EN INTERNET Y NUEVA ECONOMIA \\n ...   \n",
       "\n",
       "                                            citations headers   \n",
       "7                                                 NaN     NaN  \\\n",
       "18  [https://www.fundacionaquae.org/wiki-aquae/inn...     NaN   \n",
       "45                                                NaN     NaN   \n",
       "46                                                NaN     NaN   \n",
       "47                                                NaN     NaN   \n",
       "\n",
       "                  author  \n",
       "7         [L√ìPEZ, Tomas]  \n",
       "18  [Gonzalez, Gabriela]  \n",
       "45    [Melanie Blejter ]  \n",
       "46    [Melanie Blejter ]  \n",
       "47    [Melanie Blejter ]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.author.notnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_paragraphs(string):\n",
    "    \"\"\"\n",
    "    Corrects the paragraph by validating \\n\n",
    "    \"\"\"\n",
    "    string = re.compile('\\s*\\n').sub('', string, 1)\n",
    "    string = re.sub('(\\w\\s)\\n\\s(\\w)', r'\\1\\2',string)\n",
    "    return string\n",
    "\n",
    "df['text'] = df['text'].apply(correct_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>citations</th>\n",
       "      <th>headers</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lopez Tomas - TP 6 - Sistemas Emergentes</td>\n",
       "      <td>Marketing en Internet y Nueva Economia Trabaj...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[L√ìPEZ, Tomas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TP 0 Gabriela Gonzalez MKTG y NV Econom√≠a</td>\n",
       "      <td>TRABAJO PRACTICO 0 Integrante ‚Äì AnÃÉo 2019 Pro...</td>\n",
       "      <td>[https://www.fundacionaquae.org/wiki-aquae/inn...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Gonzalez, Gabriela]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>TP N¬∞ 1 ‚Äì WIKINOMICS - Melanie Blejter</td>\n",
       "      <td>MARKETING EN INTERNET Y NUEVA ECONOMIA Catedr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Melanie Blejter ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>TP N¬∞ 2 ‚Äì La larga cola - Melanie Blejter</td>\n",
       "      <td>MARKETING EN INTERNET Y NUEVA ECONOMIA Docent...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Melanie Blejter ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>TP N¬∞ 3 ‚Äì The Experience Economy - Melanie Ble...</td>\n",
       "      <td>MARKETING EN INTERNET Y NUEVA ECONOMIA Catedr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Melanie Blejter ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filename   \n",
       "7            Lopez Tomas - TP 6 - Sistemas Emergentes  \\\n",
       "18          TP 0 Gabriela Gonzalez MKTG y NV Econom√≠a   \n",
       "45             TP N¬∞ 1 ‚Äì WIKINOMICS - Melanie Blejter   \n",
       "46          TP N¬∞ 2 ‚Äì La larga cola - Melanie Blejter   \n",
       "47  TP N¬∞ 3 ‚Äì The Experience Economy - Melanie Ble...   \n",
       "\n",
       "                                                 text   \n",
       "7    Marketing en Internet y Nueva Economia Trabaj...  \\\n",
       "18   TRABAJO PRACTICO 0 Integrante ‚Äì AnÃÉo 2019 Pro...   \n",
       "45   MARKETING EN INTERNET Y NUEVA ECONOMIA Catedr...   \n",
       "46   MARKETING EN INTERNET Y NUEVA ECONOMIA Docent...   \n",
       "47   MARKETING EN INTERNET Y NUEVA ECONOMIA Catedr...   \n",
       "\n",
       "                                            citations headers   \n",
       "7                                                 NaN     NaN  \\\n",
       "18  [https://www.fundacionaquae.org/wiki-aquae/inn...     NaN   \n",
       "45                                                NaN     NaN   \n",
       "46                                                NaN     NaN   \n",
       "47                                                NaN     NaN   \n",
       "\n",
       "                  author  \n",
       "7         [L√ìPEZ, Tomas]  \n",
       "18  [Gonzalez, Gabriela]  \n",
       "45    [Melanie Blejter ]  \n",
       "46    [Melanie Blejter ]  \n",
       "47    [Melanie Blejter ]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.author.notnull()].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_contiguos_persons(listed_doc):\n",
    "    \"\"\"\n",
    "    Return the start and end index of the first contiguos persons in a list of spacy spans\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc_ents = [span.ent_type_ for span in listed_doc]\n",
    "\n",
    "        #Looks for the index of the first PER entity\n",
    "        index_start = doc_ents.index('PER')\n",
    "\n",
    "        #Loop through entities until it finds a non PER entity\n",
    "        for i, slice in enumerate(doc_ents[index_start:]):\n",
    "            index_end = index_start + i\n",
    "            if slice != 'PER':\n",
    "                break\n",
    "        return (index_start, index_end)\n",
    "    \n",
    "    except:\n",
    "\n",
    "        #If there are not any PER entities, return -1\n",
    "        index_start = -1\n",
    "        index_end = -1\n",
    "        return (index_start, index_end)\n",
    "\n",
    "def slice_string(string):\n",
    "    \"\"\"\n",
    "    Receives a string a return it sliced where the author is supposed to be\n",
    "    \"\"\"\n",
    "    string = string.split('\\n')\n",
    "    i = 0\n",
    "    while(i<len(string)):\n",
    "        slice = string[i]\n",
    "        slice = slice.lower().strip()\n",
    "        for synonym in author_synonyms:\n",
    "\n",
    "            #Joining the author synonym is in another line that the author name and we need some context for entities recognition\n",
    "            if slice.endswith(synonym) or slice.endswith(synonym + \":\"):\n",
    "                return ' '.join(string[i: i+2])\n",
    "            \n",
    "            #If the author is in the same line as the synonym\n",
    "            if synonym in slice:\n",
    "                return slice\n",
    "        i +=1\n",
    "    return ' '.join(string)\n",
    "\n",
    "def get_author(string, headers):\n",
    "    \"\"\"\n",
    "    Look for authors in a documents and return a list of them.\n",
    "    If it has trouble finding the author, it a list of first contiguos persons entities.\n",
    "    Also if there are not any persons entities, it returns np.nan\n",
    "    \"\"\"\n",
    "    stop_author = ['legajo', 'email', 'mail', 'correo electronico', 'e-mail']\n",
    "    author_synonyms = ['nombre','nombres','apellido','apellidos','nombre y apellido','apellido y nombre','nombres y apellidos','apellidos y nombres','alumno','alumnos', 'alumna','alumne','alumnes']\n",
    "    string = slice_string(string)\n",
    "    author = [] \n",
    "    doc = nlp(string)\n",
    "    author_synonyms = [author for author in author_synonyms if author in doc.text.lower()]\n",
    "\n",
    "    for i,token in enumerate(doc):\n",
    "        #If there is any author synonym, look for the first contiguos persons before it\n",
    "        if token.text.lower() in author_synonyms:\n",
    "            sliced_doc = doc[i+1:]\n",
    "            doc_list = [doc for doc in sliced_doc if (doc.text.lower() not in stop_author) and not(doc.pos_ == 'PUNCT' or doc.pos_ == 'SPACE')]\n",
    "            index = first_contiguos_persons(doc_list)\n",
    "            author += [doc.text for doc in doc_list[index[0]:index[1]]]\n",
    "\n",
    "            return author\n",
    "    \n",
    "    #If there are not any author synonyms, look into haeders the first contiguos persons\n",
    "    if author == [] and not (type(headers) == float and pd.isna(headers)):\n",
    "        \n",
    "        for header in headers:\n",
    "            doc = nlp(header)\n",
    "            for i, token in enumerate(doc):\n",
    "                if token.text.lower() in author_synonyms:\n",
    "                    doc = doc[i+1:]\n",
    "                    doc_list = [token for token in doc if (doc.text.lower() not in stop_author) and not(doc.pos_ == 'PUNCT' or doc.pos_ == 'SPACE')]\n",
    "                    index = first_contiguos_persons(doc_list)\n",
    "                    author += [doc.text for doc in doc_list[index[0]:index[1]]]\n",
    "    elif author == []:\n",
    "\n",
    "        author = np.nan\n",
    "    \n",
    "    return author\n",
    "\n",
    "df['author'] = df.apply(lambda x : get_author(x['text'], x['headers']),axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(string):\n",
    "    \"\"\"\n",
    "    Clean a string from special characters and numbers\n",
    "    \"\"\"\n",
    "    string = re.sub('([a-zA-Z])-([a-zA-Z])', r'\\1\\2',string)\n",
    "    string = string.replace('\\n',' ')\n",
    "    string = re.sub('‚óè|‚Ä¢|-|‚Äù|‚Äú|¬∞|,|/|:|\\?|¬ø|!|¬°',' ', string)\n",
    "    string = string.replace('(',' ').replace(')',' ').replace('[',' ').replace(']',' ').replace('{',' ').replace('}',' ')\n",
    "    string = re.sub('\\d', ' ', string)\n",
    "    string = re.sub('\\s+',' ',string)\n",
    "    string = string.strip()\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_topic(text):\n",
    "    \"\"\"\n",
    "    Get the most common words in a corpus\n",
    "    \"\"\"\n",
    "    text = clean_sentences(text)\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and token.pos_ != 'SPACE' and token.pos_ != 'PUNCT' and len(token.text)>1]\n",
    "    topic = Counter(tokens).most_common(10)\n",
    "    topic = [token[0] for token in topic]\n",
    "    return topic\n",
    "\n",
    "df['topic'] = df['text'].apply(get_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sistema', 'emergente', 'complejo', 'describa', 'estudio', 'conducta', 'moho', 'fango', 'organismo', 'forma']\n"
     ]
    }
   ],
   "source": [
    "print(df['topic'].iloc[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking for topics in Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_topics(topic):\n",
    "    \"\"\"\n",
    "    Look for topics in google and return the first 3 results\n",
    "    \"\"\"\n",
    "    query = ' '.join(topic)\n",
    "    urls = [url for url in search(query, num_results=3)][0:3]\n",
    "    return urls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_dots(string):\n",
    "    \"\"\"\n",
    "    Only keeping dots for sentences separation\n",
    "    \"\"\"\n",
    "    string = re.sub('([a-zA-Z]\\s?)\\.(\\s?[a-z])', r'\\1 \\2', string)\n",
    "    string = re.sub('([a-zA-Z]\\s?)\\.(\\s?\\))', r'\\1 \\2', string)\n",
    "    string = re.sub('(\\s[A-Z]\\s?)\\.(\\s?[a-zA-Z])', r'\\1 \\2', string)\n",
    "    string = string.replace('\\n', ' ')\n",
    "    return string\n",
    "\n",
    "def get_corpus(text):\n",
    "    \"\"\"\n",
    "    Get a list of sentences well separated from a text\n",
    "    \"\"\"\n",
    "    text = correct_dots(text)\n",
    "    corpus = text.split('.')\n",
    "\n",
    "    return corpus\n",
    "\n",
    "def process_indexed_corpus(corpus):\n",
    "    \"\"\"\n",
    "    Receive a list of sentences and return a list of cleaned sentences with each index\n",
    "    \"\"\"\n",
    "    processed_corpus = []\n",
    "    for i, sentence in enumerate(corpus):\n",
    "        sentence = clean_sentences(sentence)\n",
    "        sentence = nlp(sentence)\n",
    "        sentence = [token.lemma_.lower() for token in sentence if not token.is_stop and token.pos_ != 'SPACE' and token.pos_ != 'PUNCT' and len(token.text)>1]\n",
    "        if len(sentence) > 3:\n",
    "            sentence = ' '.join(sentence)\n",
    "            processed_corpus.append((i, sentence))\n",
    "    return processed_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['corpus'] = df['text'].apply(get_corpus)\n",
    "df['processed_corpus'] = df['corpus'].apply(process_indexed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>citations</th>\n",
       "      <th>headers</th>\n",
       "      <th>author</th>\n",
       "      <th>topic</th>\n",
       "      <th>corpus</th>\n",
       "      <th>processed_corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EconomiÃÅa de experiencia (1)</td>\n",
       "      <td>Marketing en internet y nueva economia Economi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Pablo, Gabriel]</td>\n",
       "      <td>[cliente, experiencia, personalizacion, produc...</td>\n",
       "      <td>[Marketing en internet y nueva economia Econom...</td>\n",
       "      <td>[(0, marketing internet economia economia expe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EconomiÃÅa de experiencia</td>\n",
       "      <td>Marketing en internet y nueva economia Economi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Pablo, Gabriel]</td>\n",
       "      <td>[cliente, experiencia, personalizacion, produc...</td>\n",
       "      <td>[Marketing en internet y nueva economia Econom...</td>\n",
       "      <td>[(0, marketing internet economia economia expe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K5071 - Matias David Choren - TP N6 Sistemas E...</td>\n",
       "      <td>MARKETING EN INTERNET Y NUEVA ECONOMIA 2¬∞ CUA...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[matias, david, choren]</td>\n",
       "      <td>[sistema, emergente, complejo, describa, estud...</td>\n",
       "      <td>[ MARKETING EN INTERNET Y NUEVA ECONOMIA 2¬∞ CU...</td>\n",
       "      <td>[(0, marketing internet economia cuatrimestre ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K5071 - Matias David Choren - TP N6 Sistemas E...</td>\n",
       "      <td>MARKETING EN INTERNET Y NUEVA ECONOMIA 2¬∞ CUA...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[matias, david, choren]</td>\n",
       "      <td>[sistema, emergente, complejo, describa, estud...</td>\n",
       "      <td>[ MARKETING EN INTERNET Y NUEVA ECONOMIA 2¬∞ CU...</td>\n",
       "      <td>[(0, marketing internet economia cuatrimestre ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K5071 - Matias David Choren - TP N¬∞5 Rifkin (1)</td>\n",
       "      <td>MARKETING EN INTERNET Y NUEVA ECONOMIA 2¬∞ CUA...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[matias, david, choren]</td>\n",
       "      <td>[rifkin, produccion, actual, economia, revoluc...</td>\n",
       "      <td>[ MARKETING EN INTERNET Y NUEVA ECONOMIA 2¬∞ CU...</td>\n",
       "      <td>[(0, marketing internet economia cuatrimestre ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>Trabajo PraÃÅctico 2 - Hernan Dalle Nogare</td>\n",
       "      <td>Trabajo Practico 2 Hernan Dalle Nogare - 146.8...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[marketing, empresa, cliente, cambio, consumid...</td>\n",
       "      <td>[Trabajo Practico 2 Hernan Dalle Nogare - 146,...</td>\n",
       "      <td>[(0, trabajo practico hernan dalle nogare), (2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>Trabajo PraÃÅctico 3 - Hernan Dalle Nogare (1)</td>\n",
       "      <td>Trabajo Practico 3 Hernan Dalle Nogare - 146.8...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[experiencia, cliente, personalizacion, produc...</td>\n",
       "      <td>[Trabajo Practico 3 Hernan Dalle Nogare - 146,...</td>\n",
       "      <td>[(0, trabajo practico hernan dalle nogare), (1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Trabajo PraÃÅctico 3 - Hernan Dalle Nogare (2)</td>\n",
       "      <td>Trabajo Practico 3 Hernan Dalle Nogare - 146.8...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[experiencia, cliente, personalizacion, produc...</td>\n",
       "      <td>[Trabajo Practico 3 Hernan Dalle Nogare - 146,...</td>\n",
       "      <td>[(0, trabajo practico hernan dalle nogare), (1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>Trabajo PraÃÅctico 4 - Hernan Dalle Nogare</td>\n",
       "      <td>Trabajo Practico 4 Hernan Dalle Nogare - 146.8...</td>\n",
       "      <td>[https://datosmacro.expansion.com/pib, https:/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[online, venta, ecommerce, internet, usuario, ...</td>\n",
       "      <td>[Trabajo Practico 4 Hernan Dalle Nogare - 146,...</td>\n",
       "      <td>[(0, trabajo practico hernan dalle nogare), (1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>Trabajo PraÃÅctico 5 - Hernan Dalle Nogare</td>\n",
       "      <td>Trabajo Practico 5 Hernan Dalle Nogare - 146.8...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[permitir, modelo, producto, ejemplo, platafor...</td>\n",
       "      <td>[Trabajo Practico 5 Hernan Dalle Nogare - 146,...</td>\n",
       "      <td>[(0, trabajo practico hernan dalle nogare), (1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>307 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              filename   \n",
       "0                         EconomiÃÅa de experiencia (1)  \\\n",
       "1                             EconomiÃÅa de experiencia   \n",
       "2    K5071 - Matias David Choren - TP N6 Sistemas E...   \n",
       "3    K5071 - Matias David Choren - TP N6 Sistemas E...   \n",
       "4      K5071 - Matias David Choren - TP N¬∞5 Rifkin (1)   \n",
       "..                                                 ...   \n",
       "302          Trabajo PraÃÅctico 2 - Hernan Dalle Nogare   \n",
       "303      Trabajo PraÃÅctico 3 - Hernan Dalle Nogare (1)   \n",
       "304      Trabajo PraÃÅctico 3 - Hernan Dalle Nogare (2)   \n",
       "305          Trabajo PraÃÅctico 4 - Hernan Dalle Nogare   \n",
       "306          Trabajo PraÃÅctico 5 - Hernan Dalle Nogare   \n",
       "\n",
       "                                                  text   \n",
       "0    Marketing en internet y nueva economia Economi...  \\\n",
       "1    Marketing en internet y nueva economia Economi...   \n",
       "2     MARKETING EN INTERNET Y NUEVA ECONOMIA 2¬∞ CUA...   \n",
       "3     MARKETING EN INTERNET Y NUEVA ECONOMIA 2¬∞ CUA...   \n",
       "4     MARKETING EN INTERNET Y NUEVA ECONOMIA 2¬∞ CUA...   \n",
       "..                                                 ...   \n",
       "302  Trabajo Practico 2 Hernan Dalle Nogare - 146.8...   \n",
       "303  Trabajo Practico 3 Hernan Dalle Nogare - 146.8...   \n",
       "304  Trabajo Practico 3 Hernan Dalle Nogare - 146.8...   \n",
       "305  Trabajo Practico 4 Hernan Dalle Nogare - 146.8...   \n",
       "306  Trabajo Practico 5 Hernan Dalle Nogare - 146.8...   \n",
       "\n",
       "                                             citations headers   \n",
       "0                                                  NaN     NaN  \\\n",
       "1                                                  NaN     NaN   \n",
       "2                                                  NaN     NaN   \n",
       "3                                                  NaN     NaN   \n",
       "4                                                  NaN     NaN   \n",
       "..                                                 ...     ...   \n",
       "302                                                NaN     NaN   \n",
       "303                                                NaN     NaN   \n",
       "304                                                NaN     NaN   \n",
       "305  [https://datosmacro.expansion.com/pib, https:/...     NaN   \n",
       "306                                                NaN     NaN   \n",
       "\n",
       "                      author   \n",
       "0           [Pablo, Gabriel]  \\\n",
       "1           [Pablo, Gabriel]   \n",
       "2    [matias, david, choren]   \n",
       "3    [matias, david, choren]   \n",
       "4    [matias, david, choren]   \n",
       "..                       ...   \n",
       "302                      NaN   \n",
       "303                      NaN   \n",
       "304                      NaN   \n",
       "305                      NaN   \n",
       "306                      NaN   \n",
       "\n",
       "                                                 topic   \n",
       "0    [cliente, experiencia, personalizacion, produc...  \\\n",
       "1    [cliente, experiencia, personalizacion, produc...   \n",
       "2    [sistema, emergente, complejo, describa, estud...   \n",
       "3    [sistema, emergente, complejo, describa, estud...   \n",
       "4    [rifkin, produccion, actual, economia, revoluc...   \n",
       "..                                                 ...   \n",
       "302  [marketing, empresa, cliente, cambio, consumid...   \n",
       "303  [experiencia, cliente, personalizacion, produc...   \n",
       "304  [experiencia, cliente, personalizacion, produc...   \n",
       "305  [online, venta, ecommerce, internet, usuario, ...   \n",
       "306  [permitir, modelo, producto, ejemplo, platafor...   \n",
       "\n",
       "                                                corpus   \n",
       "0    [Marketing en internet y nueva economia Econom...  \\\n",
       "1    [Marketing en internet y nueva economia Econom...   \n",
       "2    [ MARKETING EN INTERNET Y NUEVA ECONOMIA 2¬∞ CU...   \n",
       "3    [ MARKETING EN INTERNET Y NUEVA ECONOMIA 2¬∞ CU...   \n",
       "4    [ MARKETING EN INTERNET Y NUEVA ECONOMIA 2¬∞ CU...   \n",
       "..                                                 ...   \n",
       "302  [Trabajo Practico 2 Hernan Dalle Nogare - 146,...   \n",
       "303  [Trabajo Practico 3 Hernan Dalle Nogare - 146,...   \n",
       "304  [Trabajo Practico 3 Hernan Dalle Nogare - 146,...   \n",
       "305  [Trabajo Practico 4 Hernan Dalle Nogare - 146,...   \n",
       "306  [Trabajo Practico 5 Hernan Dalle Nogare - 146,...   \n",
       "\n",
       "                                      processed_corpus  \n",
       "0    [(0, marketing internet economia economia expe...  \n",
       "1    [(0, marketing internet economia economia expe...  \n",
       "2    [(0, marketing internet economia cuatrimestre ...  \n",
       "3    [(0, marketing internet economia cuatrimestre ...  \n",
       "4    [(0, marketing internet economia cuatrimestre ...  \n",
       "..                                                 ...  \n",
       "302  [(0, trabajo practico hernan dalle nogare), (2...  \n",
       "303  [(0, trabajo practico hernan dalle nogare), (1...  \n",
       "304  [(0, trabajo practico hernan dalle nogare), (1...  \n",
       "305  [(0, trabajo practico hernan dalle nogare), (1...  \n",
       "306  [(0, trabajo practico hernan dalle nogare), (1...  \n",
       "\n",
       "[307 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_urls(urls):\n",
    "    \"\"\"\n",
    "    Read the text of each hyperlink and concatenate it to the dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame(columns=['url','corpus', 'processed_corpus'])\n",
    "    for url in urls:\n",
    "            try:\n",
    "                req = Request(\n",
    "                            url=url,\n",
    "                            headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                html = urlopen(req).read()\n",
    "            except:\n",
    "                continue\n",
    "            soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "            #Kill all script and style elements\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.extract()\n",
    "\n",
    "            #Get text\n",
    "            text = soup.get_text()\n",
    "\n",
    "            #Break into lines and remove leading and trailing space on each\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            \n",
    "            #Break multi-headlines into a line each\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "\n",
    "            #Drop blank lines\n",
    "            text = '. '.join(chunk for chunk in chunks if chunk)\n",
    "            text = clean_special_characters(text)\n",
    "            corpus = get_corpus(text)\n",
    "            processed_corpus = process_indexed_corpus(corpus)\n",
    "            \n",
    "\n",
    "            df = pd.concat([df, pd.DataFrame({'url':url,'corpus':[corpus], 'processed_corpus': [processed_corpus]}, index=[0])], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plagiarism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_dictionary(dic, key, index, element):\n",
    "    dic.setdefault(key, {'n_sentence': index, 'plagiarism': []})\n",
    "    dic[key]['plagiarism'] += element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_similarities(df, comparing_df, similarities):\n",
    "    \n",
    "    unprocessed_corpus = df['corpus']\n",
    "    processed_corpus = df['processed_corpus']\n",
    "    \n",
    "    corpus = [slice[1] for slice in processed_corpus]\n",
    "    \n",
    "    for _, row in comparing_df.iterrows():\n",
    "\n",
    "\n",
    "        row_unprocessed_corpus = row['corpus']\n",
    "        row_indexed_corpus = row['processed_corpus']\n",
    "        row_corpus = [slice[1] for slice in row_indexed_corpus]\n",
    "\n",
    "        vectorizer = CountVectorizer()\n",
    "        vectorizer.fit(row_corpus + corpus)\n",
    "\n",
    "        for slice in processed_corpus:\n",
    "            index, sentence = slice\n",
    "\n",
    "            plagiarism=[]\n",
    "            for id, row_slice in enumerate(row_indexed_corpus):\n",
    "                row_index, row_sentence = row_slice\n",
    "                vector = vectorizer.transform([row_sentence, sentence])\n",
    "                score = cosine_similarity(vector)[0][1]\n",
    "                if 0.95 > score > 0.70:\n",
    "                    plagiarism.append({'plagiarized_sentence':row_unprocessed_corpus[row_index],\n",
    "                                      'plagiarism_score':score, \n",
    "                                      'plagiarized_file':row['filename'],  \n",
    "                                      'plagiarized_author':row['author'] \n",
    "                                    })\n",
    "                    \n",
    "            if len(plagiarism) > 0:\n",
    "                append_to_dictionary(similarities, unprocessed_corpus[index], index, plagiarism)\n",
    "                    \n",
    "    return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_similarities(df, urls, similarities):\n",
    "    \n",
    "\n",
    "    unprocessed_corpus = df['corpus']\n",
    "    processed_corpus = df['processed_corpus']\n",
    "    \n",
    "    comparing_df = read_urls(urls)\n",
    "\n",
    "    corpus = [slice[1] for slice in processed_corpus]\n",
    "    \n",
    "    for _, row in comparing_df.iterrows():\n",
    "\n",
    "\n",
    "        row_unprocessed_corpus = row['corpus']\n",
    "        row_indexed_corpus = row['processed_corpus']\n",
    "        row_corpus = [slice[1] for slice in row_indexed_corpus]\n",
    "\n",
    "        vectorizer = CountVectorizer()\n",
    "        vectorizer.fit(row_corpus + corpus)\n",
    "\n",
    "        for slice in processed_corpus:\n",
    "            index, sentence = slice\n",
    "\n",
    "            plagiarism=[]\n",
    "            for id, row_slice in enumerate(row_indexed_corpus):\n",
    "                row_index, row_sentence = row_slice\n",
    "                vector = vectorizer.transform([row_sentence, sentence])\n",
    "                score = cosine_similarity(vector)[0][1]\n",
    "                if 0.95 > score > 0.70:\n",
    "                    #similarity[str(id)]\n",
    "                    plagiarism.append({'plagiarized_sentence':row_unprocessed_corpus[row_index],\n",
    "                                      'plagiarism_score':score, \n",
    "                                      'plagiarized_website':row['url'] \n",
    "                                   })\n",
    "                    #similarities.append(similarity)\n",
    "            if len(plagiarism) > 0:\n",
    "                append_to_dictionary(similarities, unprocessed_corpus[index], index, plagiarism)\n",
    "                    \n",
    "                    \n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def googled_topic_similarities(df, topic, similarities):\n",
    "    \n",
    "    urls = google_topics(topic)\n",
    "    return url_similarities(df, urls, similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarities(df, comparing_df):\n",
    "    similarities = {}\n",
    "    db_similarities(df[['corpus','processed_corpus']], comparing_df, similarities)\n",
    "\n",
    "    if df['citations'] == True:\n",
    "        url_similarities(df[['corpus', 'processed_corpus']], df['citations'],similarities)\n",
    "        \n",
    "    googled_topic_similarities(df[['corpus', 'processed_corpus']],df['topic'],similarities)\n",
    "    return similarities\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.iloc[273]\n",
    "\n",
    "text = df_test['text']\n",
    "\n",
    "topic = get_topic(text)\n",
    "corpus = get_corpus(text)\n",
    "processed_corpus = process_indexed_corpus(corpus)\n",
    "\n",
    "#Filtering by the intersection of topics\n",
    "filtered_df = df[df['topic'].apply(lambda x: len(set(x) & set(topic)) > 3)][df['filename'] != df_test['filename']]\n",
    "\n",
    "similarity = get_similarities(df_test, filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('results.json', 'w') as results_file:\n",
    "    json.dump(similarity, results_file, indent=4, sort_keys=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
